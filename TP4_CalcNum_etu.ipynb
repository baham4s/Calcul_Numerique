{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4: Dichotomie et Descente de Gradient\n",
    "\n",
    "Dans ce TP nous allons implémenter les méthodes vues en cours pour la recherche de minimum d'une fonction $h(x)$. Cette recherche de minimum peut aussi être vu comme la recherche des zéros de la dérivée, soit la résolution de l'équation $h'(x)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit les fonctions suivantes:\n",
    "* $f(x) = \\ln x + x^2$\n",
    "* $g(x,y) = \\sin(y) + x^2$ (déjà vue dans les TPs précédents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1: Etude de la fonction $f$\n",
    "\n",
    "1) Analyser la fonction $f$\n",
    "- [x] donner son domaine de définition ==> ]0, +∞]\n",
    "- [ ] à partir du calcul de sa dérivée, étudier les variations de $f$ (croissante, décroissante) ==> (1 + 2x^2)/x\n",
    "- [ ] admet-elle un minima ?\n",
    "- [ ] donner les limites de $f$ aux bornes de son domaine de définition.\n",
    "- [ ] la fonction s'annule-t-elle sur cet intervalle ?\n",
    "- [ ] déterminer $f(1)$, qu'en concluez-vous ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Représenter la fonction $f$ sur son domaine de définition. On pourra utiliser `N=1000` points. Vous choisirez une borne maximum intéressante pour visualiser le point d'annulation.\n",
    "\n",
    "**Attention** à vérifier que votre fonction logarithme est bien le logarithme népérien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eps = 1e-3\n",
    "N = 1000\n",
    "def f(x):\n",
    "    return np.log(x) + x**2\n",
    "\n",
    "def fDev(x):\n",
    "    return (1+2 * x**2)/x\n",
    "\n",
    "def tracer(x):\n",
    "    #plt.figure(figsize=(12,6))\n",
    "    plt.plot(x, f(x))\n",
    "    #plt.title('tracé de la fonction x entre 0 et 10')\n",
    "    plt.show()\n",
    "\n",
    "tracer(np.linspace(eps, 10, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: Dichotomie\n",
    "\n",
    "Nous avons vu en cours, que la dichotomie permettait de résoudre des équations du type $h(x)=0$ en encadrant la solution par deux valeurs $a$ et $b$ telles que $f(a) \\cdot f(b) \\leq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Proposer un programme qui prend en entrée deux réels $a< b$ et une fonction $f$ dont on cherche à déterminer la racine dans l'intervalle $[a,b]$ par dichotomie. Ce programme construit à chaque itération un nouvel intervalle $I_n$ dans lequel se trouve la solution, et une une suite $(x_n)$ qui converge vers la solution.\n",
    "- [ ] penser à intialiser votre intervalle et votre suite\n",
    "- [ ] tester deux approches, la première où l'intervalle $I_n$ est coupé en deux aléatoirement, la seconde où cet intervalle est coupé en sa moitié.\n",
    "- [ ] ajouter une tolérance sur votre racine.\n",
    "- [ ] _bonus_ ajouter un nombre d'itération maximum (pour éviter de tourner en rond...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dichotomie : (10, 0.6536669921875)\n",
      "Sécante : (3, 0.6529186296480068)\n",
      "Newton : (10, 0.6529141145063018)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dichtomie(a, b):\n",
    "    i = 0\n",
    "    delta = (a + b) / 2\n",
    "    while np.abs(a-b) > eps:\n",
    "        delta = (a + b) / 2\n",
    "        if f(a) * f(delta) <= 0:\n",
    "            b = delta\n",
    "        else:\n",
    "            a = delta\n",
    "        i+=1\n",
    "    return i, delta\n",
    "\n",
    "\n",
    "def secante(a, b):\n",
    "    a, b = b, a - (b-a) * f(a) / (f(b) - f(a))\n",
    "    i=0\n",
    "    while np.abs(a - b) > eps:\n",
    "        a, b = b, a - (b-a) * f(a)/ (f(b) - f(a))\n",
    "        i+=1\n",
    "    return i, b\n",
    "\n",
    "\n",
    "def newton(a, b):\n",
    "    a, b = b, a - (f(a) / fDev(a))\n",
    "    i=0\n",
    "    while np.abs(a - b) > eps:\n",
    "        a, b = b, a - (f(a) / fDev(a))\n",
    "        i+=1\n",
    "    return i, b\n",
    "\n",
    "\n",
    "print(f\"Dichotomie : {dichtomie(eps, 1)}\")\n",
    "print(f\"Sécante : {secante(eps, 1)}\")\n",
    "print(f\"Newton : {newton(eps, 1)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Trouver une (ou plusieurs) solutions approchées de l'équation $f(x)=0$ en utilisant la méthode par dichotomie avec une précision de 0.001\n",
    "- [x] déterminer théoriquement le nombre d'itérations nécessaires pour obtenir cette précision. Vérifier cette valeur expérimentalement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Bonus** Créer maintenant un programme qui permet d'obtenir la racine grâce à la méthode de la sécante.\n",
    "- [x] comparer le nombre d'itérations prises avec cette méthode par rapport à la dichotomie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) **Bonus** Créer maintenant un programme qui permet d'obtenir la racine grâce à la méthode de Newton.\n",
    "- [x] comparer le nombre d'itérations prises avec cette méthode par rapport aux précédentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3: Descente de Gradient\n",
    "\n",
    "Nous allons maintenant travailler sur la fonction en 2D $g(x)$.\n",
    "On rappelle que le calcul d'une dérivée partielle peut être soit définie analytiquement avec une formule mathématique (vu dans le TP1), soit obtenue grâce aux accroissements finis. Dans ce TP, nous allons utiliser l'approximation suiante pour le calcul de la dérivée parielle suivant la variable $x$ au point $(x_0, y_0)$:\n",
    "\n",
    "$$\\dfrac{\\partial f}{\\partial x}(x_0,x_0) \\approx \\dfrac{f(x_0+ \\epsilon,y_0) - f(x_0-\\epsilon, y_0)}{2\\epsilon}$$\n",
    "\n",
    "On rappelle également que le gradient de la fonction $g$ au point $(x_0, y_0)$ s'exprime:\n",
    "\n",
    "$$\\nabla g(x_0, y_0) = \\left(\\dfrac{\\partial g}{\\partial x}(x_0, y_0); \\dfrac{\\partial g}{\\partial y}(x_0, y_0)\\right)$$\n",
    "\n",
    "Si on cherche un minimum de la fonction, on va alors chercher les conditions du 1er ordre (CPO) et annuler le gradient pour déterminer le point $(x_0, y_0)$:\n",
    "$$\\nabla g(x_0, y_0) = \\mathbf{0}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Écrire une fonction qui calcule le gradient d'une fonction de plusieurs variables par différences finies au point $\\mathbf{x}$\n",
    "- [ ] vérifier que votre fonction fonctionne avec plusieurs dimensions.\n",
    "- [ ] vérifier que si $\\mathbf{x} \\in \\mathbb{R}^d$ alors votre fonction retourne bien un vecteur de taille $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient à 1 dimension : -0.2497634794112833\n",
      "1 --> [1.4933333323456797, 1.9600000000000044]\n",
      "2 --> [1.4866369027688817, 1.9208000000000052]\n",
      "3 --> [1.4799103096851205, 1.8823840000000063]\n",
      "4 --> [1.4731531424050957, 1.8447363200000093]\n",
      "5 --> [1.4663649808432608, 1.807841593600013]\n",
      "6 --> [1.459545395214262, 1.7716847617280167]\n",
      "7 --> [1.4526939457166774, 1.736251066493462]\n",
      "8 --> [1.4458101822033607, 1.7015260451635976]\n",
      "9 --> [1.4388936438377407, 1.66749552426033]\n",
      "10 --> [1.4319438587352704, 1.634145613775127]\n",
      "11 --> [1.4249603435893063, 1.6014627014996279]\n",
      "12 --> [1.4179426032805251, 1.5694334474696374]\n",
      "13 --> [1.410890130469019, 1.5380447785202471]\n",
      "14 --> [1.4038024051681095, 1.507283882949844]\n",
      "15 --> [1.3966788942988693, 1.4771382052908493]\n",
      "16 --> [1.3895190512242737, 1.447595441185035]\n",
      "17 --> [1.38232231526184, 1.4186435323613371]\n",
      "18 --> [1.3750881111735112, 1.390270661714113]\n",
      "19 --> [1.367815848631496, 1.3624652484798343]\n",
      "20 --> [1.3605049216586438, 1.3352159435102422]\n",
      "21 --> [1.353154708041878, 1.3085116246400412]\n",
      "Gradient à 2 dimension : 2.0146413589207026\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def g(x):\n",
    "    return 2 * x ** 2 + x + 3\n",
    "\n",
    "def f(t):\n",
    "    # T0 --> x; T1 --> y\n",
    "    # ln.x  +  y^2\n",
    "    return np.log(t[0]) + t[1]**2\n",
    "\n",
    "# Descente de Gradient a 1 dimension\n",
    "def grad(f, a):\n",
    "    g = 1\n",
    "    while abs(g) > eps:\n",
    "        g = (f(a + eps) - f(a - eps)) / (2 * eps)\n",
    "        a = a - 0.01 * g\n",
    "    return a\n",
    "\n",
    "# Descente de gradient a 2 dimension\n",
    "def grad_bis(t=None):\n",
    "    if t is None:\n",
    "        t = [1.5, 2]\n",
    "    x, y = t[0], t[1]\n",
    "    i = 0\n",
    "    while np.linalg.norm([x, y]) > eps:\n",
    "        if i > 20: break\n",
    "        x = x - 0.01 * (f([x + eps, y]) - f([x - eps, y])) / (2 * eps)\n",
    "        y = y - 0.01* (f([x, y + eps]) - f([x, y - eps])) / (2 * eps)\n",
    "        i+=1\n",
    "        print(f\"{i} --> {[x, y]}\")\n",
    "\n",
    "    return f(np.array([x, y]))\n",
    "\n",
    "print(f\"Gradient à 1 dimension : {grad(g, 1)}\")\n",
    "print(f\"Gradient à 2 dimension : {grad_bis()}\")\n",
    "#np.diff(f, axis = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Écrire l'algorithme de descente pour une fonction à deux variables. On considèrera que l'algorithme a trouvé la solution si la norme du gradient est proche de 0 soit par exemple : `np.linalg.norm(G) < tol`. \n",
    "- [-] ajouter un seuil de tolérance en entrée de l'algorithme.\n",
    "- [x] définir un nombre d'itération maximum `Niter` tel que l'algorithme s'arrête après `Niter` même si il n'a pas approché suffisamment la solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Afin de pouvoir visualiser l'algorithme de descente, modifier votre algorithme de descente afin qu'il retourne non seulement la valeur de minimum trouvé, mais aussi la suite des valeurs $(\\mathbf{x}_n)$ par lesquelles il est passé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Choisir un point pour l'initialisation et une valeur de vitesse d'apprentissage. On pourra prendre  $\\mathbf{x}_0 = (1.5, 2)$ et $\\alpha \\in ]0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Afin de visualiser l'algorithme de descente, tracer les contours de $g$ sur un plan en 2D. Et afficher la suite des valeurs $(\\mathbf{x}_n)$ par lesquelles l'algorithme est passé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour():\n",
    "    x = ??\n",
    "    y = ??\n",
    "    x,y = np.meshgrid(x, y)\n",
    "    z = x∗∗2 + np.sin(y)\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    axes = fig.gca()\n",
    "    axes.contour(x,y, z, 21) \n",
    "    axes.plot(...., ’−+r’) \n",
    "    axes.set_title(’z = x^2 + sin(y)’) \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Faites varier le point de départ $\\mathbf{x}_0$ puis la vitesse d'apprentissage $\\alpha$. Observer les effets induits sur la valeur de la solution à l'aide du graphique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4: application à l'apprentissage automatique\n",
    "\n",
    "Dans le contexte de l'apprentissage automatique, les minima correspondent aux valeurs des paramètres du modèle que l'on optimise via une fonction de coût.\n",
    "\n",
    "Voici ci-dessous un tableau d'équivalence de l'utilisation de la descente de gradient dans le cas de l'apprentissage automatique (par exemple régression linéaire ou logistique) et le cas que nous venons de traiter.\n",
    "\n",
    "| Vu en TP | App Auto |\n",
    "| - | - | \n",
    "| vecteur | vecteur de paramètres du modèle |\n",
    "|minimum local | paramètres optimum |\n",
    "|fonction g | fonction de coût J |\n",
    "| ?? | données d'apprentissage |\n",
    "\n",
    "- [ ] à quoi correspondent les données d'apprentissage dans ce que nous venons de voir en TP ?\n",
    "- [x] donner la fonction de coût pour la regression linéaire.\n",
    "- --> La moyenne de toute les erreurs sur le nombre de données (Erreur quadratique moyenne)\n",
    "- [x] connaissez-vous d'autres fonctions de coût ?\n",
    "- --> Méthode des moindres carrées\n",
    "- --> Square Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5: Multiplication de valeurs proches de 0\n",
    "\n",
    "1) Est-ce que l'égalité suivante : `0.1 + 0.1 + 0.1 == 0.3` est vraie en Python?\n",
    "--> False\n",
    "--> A cause de la précision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) La fraction décimale `0.125` peut s'écrire sous la forme $$0 \\cdot 10^0 + 1 \\cdot 10^{-1} + 2 \\cdot 10^{-2} + 5 \\cdot 10^{-3}$$\n",
    "De la même manière, la fraction binaire `0.001` peut s'écrire sous la forme`\n",
    "$$0 \\cdot 2^0 + 0 \\cdot 2^{-1} + 0 \\cdot 2^{-2} + 1 \\cdot 2^{-3}$$\n",
    "\n",
    "- [x] donner la forme des fractions binaires `0.1` et `0.3`\n",
    "--> 0.1 = 1/10 $$0 \\cdot 2^0 + 1 \\cdot 2^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) En utilisant le format d'affichage des flottants en python, par exemple `'\\%.2f' \\% 0.1` donner la valeur exacte en décimale de l'approximation en binaire stockée en machine pour `0.1`\n",
    "--> >>> print('%.200f'% 0.1)\n",
    "0.10000000000000000555111512312578270211815834045410156250000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Expliquer, à présent, votre résultat à la première question.\n",
    "--> Lorsque on demande 0.1 l'ordinateur prend n chiffre après la virgule et par conséquent le résultat obtenue n'est pas 3 mais supérieur à 3\n",
    "5) Que se passe-t-il lorsque l'on multiplie deux valeurs proches de 0 à l'aide d'un ordinateur?<br>\n",
    "Ce type de multiplication est souvent nécessaire lorsque nous cherchons à calculer la séquence d'événements la plus probable. Comment peut-on surmonter le problème énoncé dans la question précédente\n",
    "--> En utilisant des fractions binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-6cf30de0",
   "language": "python",
   "display_name": "PyCharm (CalculNum)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}